{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quora Insincere Questions: RNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "os.chdir('/home/roman/Documents/Projects/Quora/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and tokenize (words to index)\n",
    "train = pd.read_csv('train.csv')\n",
    "tk = Tokenizer(lower = True, filters='')\n",
    "train_text = list(train['question_text'])\n",
    "tk.fit_on_texts(train_text)\n",
    "train_tokenized = tk.texts_to_sequences(train['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad input data for input in RNN, define embedding size\n",
    "max_len = 60\n",
    "max_features = 30000\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "embed_size = 100\n",
    "embedding_matrix = np.load('embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure X_train doesn't have indices > max_features:\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        if X_train[i][j] > max_features:\n",
    "            X_train[i][j] = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one_hot for y\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(train['target'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1: one LSTM layer, avg pool\n",
    "def build_model(lr=0.0, units=0, spatial_dr=0.0, kernel_size1=3,  \n",
    "                dense_units=128, dr=0.1, epochs=20):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "    #x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    x_lstm = LSTM(units, return_sequences = True)(x1)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_lstm)\n",
    "    x = BatchNormalization()(avg_pool1_lstm)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 60, 100)           3000100   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 60, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60, 40)            22560     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1312      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 3,024,822\n",
      "Trainable params: 24,578\n",
      "Non-trainable params: 3,000,244\n",
      "_________________________________________________________________\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/5\n",
      "1175509/1175509 [==============================] - 784s 667us/step - loss: 0.2214 - acc: 0.9207 - val_loss: 0.1462 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14617, saving model to best_model.hdf5\n",
      "Epoch 2/5\n",
      "1175509/1175509 [==============================] - 776s 660us/step - loss: 0.1391 - acc: 0.9479 - val_loss: 0.1389 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14617 to 0.13891, saving model to best_model.hdf5\n",
      "Epoch 3/5\n",
      "1175509/1175509 [==============================] - 920s 783us/step - loss: 0.1340 - acc: 0.9493 - val_loss: 0.1360 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13891 to 0.13602, saving model to best_model.hdf5\n",
      "Epoch 4/5\n",
      "1175509/1175509 [==============================] - 789s 671us/step - loss: 0.1311 - acc: 0.9501 - val_loss: 0.1326 - val_acc: 0.9493\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13602 to 0.13262, saving model to best_model.hdf5\n",
      "Epoch 5/5\n",
      "1175509/1175509 [==============================] - 909s 773us/step - loss: 0.1291 - acc: 0.9507 - val_loss: 0.1305 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13262 to 0.13051, saving model to best_model.hdf5\n"
     ]
    }
   ],
   "source": [
    "file_path = \"best_model.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,\n",
    "                                  save_best_only=True, mode=\"min\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "model = build_model(lr=1e-4, units=40, spatial_dr=0, kernel_size1=4, \\\n",
    "                    dense_units=32, dr=0, epochs=5)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer=Adam(lr=1e-4, decay=1e-7), metrics = [\"accuracy\"])\n",
    "model.summary()\n",
    "model.fit(X_train, y_ohe, batch_size = 512, epochs = 5, validation_split=0.1,\\\n",
    "         verbose = 1, callbacks = [check_point, early_stop])\n",
    "model = load_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1175509/1175509 [==============================] - 550s 468us/step\n",
      "130613/130613 [==============================] - 60s 463us/step\n"
     ]
    }
   ],
   "source": [
    "# Predict results\n",
    "val_split = 0.1\n",
    "n_train = int(X_train.shape[0] * (1 - val_split))\n",
    "y_train = train['target'][:n_train]\n",
    "y_val = train['target'][n_train:]\n",
    "x_train = X_train[:n_train, :]\n",
    "x_val = X_train[n_train:, :]\n",
    "y_train_pred = model.predict(x_train, verbose = 1)\n",
    "y_val_pred = model.predict(x_val, verbose = 1)\n",
    "y_train_pred_1d = np.round(y_train_pred).dot(ohe.active_features_).astype(int)\n",
    "y_val_pred_1d = np.round(y_val_pred).dot(ohe.active_features_).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual positive:    0.06186681684274642\n",
      "predicted positive: 0.038300004508685176\n",
      "precision (train/val/random): 0.67 / 0.658 / 0.064\n",
      "recall (train/val/random):    0.415 / 0.405 / 0.039\n",
      "f1 score (train/val/random):  0.512 / 0.501 / 0.048\n"
     ]
    }
   ],
   "source": [
    "# Evaluate results\n",
    "predicted_positive = np.round(np.mean(y_train_pred_1d), 3)\n",
    "y_pred_random = np.random.binomial(1, predicted_positive, y_train.shape[0])\n",
    "actual_positve = np.round(np.mean(y_train), 3)\n",
    "precision_train = np.round(metrics.precision_score(y_train, y_train_pred_1d), 3)\n",
    "precision_val = np.round(metrics.precision_score(y_val, y_val_pred_1d), 3)\n",
    "precision_random = np.round(metrics.precision_score(y_train, y_pred_random), 3)\n",
    "recall_train = np.round(metrics.recall_score(y_train, y_train_pred_1d), 3)\n",
    "recall_val = np.round(metrics.recall_score(y_val, y_val_pred_1d), 3)\n",
    "recall_random = np.round(metrics.recall_score(y_train, y_pred_random), 3)\n",
    "f1score_train = np.round(metrics.f1_score(y_train, y_train_pred_1d), 3)\n",
    "f1score_val = np.round(metrics.f1_score(y_val, y_val_pred_1d), 3)\n",
    "f1score_random = np.round(metrics.f1_score(y_train, y_pred_random), 3)\n",
    "print('actual positive:    ' + str(np.mean(y_train)))\n",
    "print('predicted positive: ' + str(np.mean(y_train_pred_1d)))\n",
    "print('precision (train/val/random): ' + str(precision_train) + ' / ' + str(precision_val) +\n",
    "      ' / ' + str(precision_random))\n",
    "print('recall (train/val/random):    ' + str(recall_train) + ' / ' + str(recall_val) +\n",
    "      ' / ' + str(recall_random))\n",
    "print('f1 score (train/val/random):  ' + str(f1score_train) + ' / ' + str(f1score_val) +\n",
    "      ' / ' + str(f1score_random))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
