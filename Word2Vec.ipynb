{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quora Insincere Questions: Train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "#from nltk import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "os.chdir('/home/roman/Documents/Projects/Quora/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['question_text'], data['target'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- train word2vec --------------- #\n",
    "# tokenization\n",
    "texts = [[word for word in document.lower().split()] for document in data['question_text']]\n",
    "\n",
    "os.chdir('../models')\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(size=100, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(texts, update=False)\n",
    "model.train(texts, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "# save and load model:\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model:\n",
    "os.chdir('../models')\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))   # <- create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = [[word for word in document.lower().split()] for document in X_train]\n",
    "X_val_texts = [[word for word in document.lower().split()] for document in X_val]\n",
    "\n",
    "X_train_texts_w2v_mean = np.array([np.mean([w2v[word] if word in w2v else np.zeros(100) for word in text ], \\\n",
    "                                          axis=0) for text in X_train_texts])\n",
    "X_val_texts_w2v_mean = np.array([np.mean([w2v[word] if word in w2v else np.zeros(100) for word in text ], \\\n",
    "                                          axis=0) for text in X_val_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_texts_w2v_mean, y_train)\n",
    "y_train_w2v_pred = clf.predict(X_train_texts_w2v_mean)\n",
    "y_val_w2v_pred = clf.predict(X_val_texts_w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Logistic Regression with Word2Vec:\n",
      "actual positive:    0.062\n",
      "predicted positive: 0.02\n",
      "precision (train/val/random): 0.587 / 0.586 / 0.063\n",
      "recall (train/val/random):    0.193 / 0.192 / 0.02\n",
      "f1 score (train/val/random):  0.29 / 0.29 / 0.031\n"
     ]
    }
   ],
   "source": [
    "# --------------- Evaluate Results --------------- #\n",
    "predicted_positive = np.round(np.mean(y_train_w2v_pred), 3)\n",
    "y_pred_random = np.random.binomial(1, predicted_positive, y_train.shape[0])\n",
    "actual_positve = np.round(np.mean(y_train), 3)\n",
    "precision_train = np.round(metrics.precision_score(y_train, y_train_w2v_pred), 3)\n",
    "precision_val = np.round(metrics.precision_score(y_val, y_val_w2v_pred), 3)\n",
    "precision_random = np.round(metrics.precision_score(y_train, y_pred_random), 3)\n",
    "recall_train = np.round(metrics.recall_score(y_train, y_train_w2v_pred), 3)\n",
    "recall_val = np.round(metrics.recall_score(y_val, y_val_w2v_pred), 3)\n",
    "recall_random = np.round(metrics.recall_score(y_train, y_pred_random), 3)\n",
    "f1score_train = np.round(metrics.f1_score(y_train, y_train_w2v_pred), 3)\n",
    "f1score_val = np.round(metrics.f1_score(y_val, y_val_w2v_pred), 3)\n",
    "f1score_random = np.round(metrics.f1_score(y_train, y_pred_random), 3)\n",
    "print('Evaluation Logistic Regression with Word2Vec:')\n",
    "print('actual positive:    ' + str(np.round(np.mean(y_train), 3)))\n",
    "print('predicted positive: ' + str(np.round(np.mean(y_train_w2v_pred), 3)))\n",
    "print('precision (train/val/random): ' + str(precision_train) + ' / ' + \\\n",
    "      str(precision_val) + ' / ' + str(precision_random))\n",
    "print('recall (train/val/random):    ' + str(recall_train) + ' / ' + \\\n",
    "      str(recall_val) + ' / ' + str(recall_random))\n",
    "print('f1 score (train/val/random):  ' + str(f1score_train) + ' / ' + \\\n",
    "      str(f1score_val) + ' / ' + str(f1score_random))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
